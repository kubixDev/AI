  # ğŸ“— Projekt 4 - sztuczne sieci neuronowe


## â­ Zadanie 1
WykorzystujÄ…c kod klasy Neuron zaimplementuj sieÄ‡ neuronowÄ… z poniÅ¼szÄ… strukturÄ… oraz dostarcz kod do wizualizacji struktury sieci.

<br>

**Dostarczony kod:**

```
import numpy as np 

class Neuron:  
    def __init__(self, n_inputs, bias = 0., weights = None):  
        self.b = bias
        if weights: self.ws = np.array(weights)
        else: self.ws = np.random.rand(n_inputs)

    def _f(self, x):                                            # Activation function (here: leaky_relu)
        return max(x*.1, x)   

    def __call__(self, xs):                                     # Calculate the neuron's output:
                return self._f(xs @ self.ws + self.b)           #   - multiply the inputs with the weights and sum the values together
                                                                #   - add the bias value
                                                                #   - then transform the value via an activation function
```

<br>

**Implementacja Neuron wraz z sieciÄ… neuronowÄ… ANN:**

```
import numpy as np
import matplotlib.pyplot as plt
import networkx as nx

##############################
# Dostarczona klasa Neuron
##############################
class Neuron:
    def __init__(self, n_inputs, bias = 0.0, weights = None):       # Konstruktor klasy Neuron
        self.b = bias

        if weights is not None:
            self.ws = np.array(weights)
        else:
            self.ws = np.random.rand(n_inputs)


    def _f(self, x):                                                # Metoda definiujaca funkcje aktywacji
        return max(0.01 * x, x)                                     # z tabelki jest to Leaky ReLU


    def __call__(self, xs):                                         # Pozwala na wywolanie obiektu klasy
        return self._f(np.dot(xs, self.ws) + self.b)                # neuron jak funkcji



########################################################
# Implementacja klasy ANN (Artificial Neural Network)
########################################################
class ANN:
    def __init__(self, layers):                                     # Konstruktor klasy ANN
        self.layers = []
        
        for i in range(1, len(layers)):
            layer = []

            for _ in range(layers[i]):
                neuron = Neuron(layers[i - 1])
                layer.append(neuron)

            self.layers.append(layer)


    def feedforward(self, inputs):                                  # Metoda, ktora przekazuje dane                          
        self.inputs = [inputs]                                      # wejsciowe przez siec neuronowa

        for layer in self.layers:
            new_inputs = []

            for neuron in layer:
                output = neuron(self.inputs[-1].reshape(-1, len(neuron.ws)))
                new_inputs.append(output)

            self.inputs.append(np.array(new_inputs))

        return self.inputs[-1]
    

    def backpropagation(self, expected_output, learning_rate):
        deltas = [expected_output - self.inputs[-1]]

        for i in reversed(range(len(self.layers))):
            layer = self.layers[i]
            next_deltas = np.zeros(len(self.inputs[i]))

            for j, neuron in enumerate(layer):
                output = self.inputs[i + 1][j]
                delta = deltas[-1][j] * (neuron._f(output) - output)
                next_deltas = next_deltas + neuron.ws * delta
                grad_w = output * delta
                grad_b = delta
                neuron.ws = neuron.ws + learning_rate * grad_w
                neuron.b = neuron.b + learning_rate * grad_b

            deltas.append(next_deltas)


    def train(self, X, y, epochs, learning_rate):
        for epoch in range(epochs):
            for xi, yi in zip(X, y):
                self.feedforward(xi)
                self.backpropagation(yi, learning_rate)
                
            print("Epoch:", epoch)



###############################
# Wizualizacja ANN z obrazka
###############################
def visualize_ann(layers):
    G = nx.DiGraph()
    pos = {}
    labels = {}
    node_id = 0


    # Tworzenie wezlow dla warstwy wejsciowej (input layer)
    for i in range(layers[0]):
        G.add_node(node_id)
        pos[node_id] = (0, -i)
        labels[node_id] = f"Input {i+1}"
        node_id = node_id + 1


    # Tworzenie wezlow dla ukrytych warstw (hidden layers)
    layer_count = len(layers) - 1

    for layer_idx, num_nodes in enumerate(layers[1:-1], start = 1):
        for i in range(num_nodes):
            G.add_node(node_id)
            pos[node_id] = (layer_idx, -i)
            labels[node_id] = f"Hidden{layer_idx} {i+1}"
            node_id = node_id + 1


    # Tworzenie wezlow dla warstwy wyjsciowej (output layer)
    for i in range(layers[-1]):
        G.add_node(node_id)
        pos[node_id] = (layer_count, -i)
        labels[node_id] = f"Output {i+1}"
        node_id = node_id + 1


    # Dodawanie krawedzi miedzy warstwami
    prev_layer_nodes = range(layers[0])
    current_layer_start = layers[0]

    for layer_size in layers[1:]:
        current_layer_nodes = range(current_layer_start, current_layer_start + layer_size)

        for prev_node in prev_layer_nodes:
            for curr_node in current_layer_nodes:
                G.add_edge(prev_node, curr_node)

        prev_layer_nodes = current_layer_nodes
        current_layer_start = current_layer_start + layer_size


    # Rysowanie sieci przy uzyciu biblioteki NetworkX
    nx.draw(G, pos, labels = labels, with_labels = True, node_size = 3000, node_color = 'lightgreen', font_size = 8)
    plt.show()



######################################
# Wybor parametrow i uzycie funkcji
######################################
input_size = 3
hidden_layer_1_size = 4
hidden_layer_2_size = 4
output_size = 1
layers = [input_size, hidden_layer_1_size, hidden_layer_2_size, output_size]

# Inicjalizacja sztucznej sieci neuronowej
ann = ANN(layers)

# Przykladowe inputy i outputy (X to liczby, a y to wyniki sum tych liczb)
X = np.array([[1, 1, 1], [1, 1, 2], [1, 1, 3], [1, 1, 4], [1, 1, 5], [1, 1, 6], [1, 1, 7]])
y = np.array([[3], [4], [5], [6], [7], [8], [9]])

# Testowy input dla ktorego bedzie zwracal wynik (reshape dla poprawnego ksztaltu macierzy)
test_input = np.array([1, 1, 8]).reshape(-1, 3)

# Rozpoczecie trenowania i zwrocenie wyniku dla testowego inputu
ann.train(X, y, epochs=5000, learning_rate=0.01)
predicted_sum = ann.feedforward(test_input).flatten()[0]
print(predicted_sum)

# Wizualizacja struktury
visualize_ann(layers)
 ```

<br>

****Importy:****

* **numpy** - do wykonywania operacji matematycznych
*  **matplotlib.pyplot** - do rysowania wykresÃ³w
*  **networkx** - do tworzenia i wizualizacji grafÃ³w

<br>

****WyjaÅ›nienie:****

> [Neuron] def  \_\_init__(self, n_inputs, bias = 0.0, weights = None):

* Jest to dostarczony konstruktor klasy Neuron
* **n_inputs** - liczba wejÅ›Ä‡ dla neuronu
* **bias** - przesuniÄ™cie neuronu, domyÅ›lnie "0.0"
* **weights** - wagi dla wejÅ›Ä‡ neuronu, domyÅ›lnie "None"
* na poczÄ…tek ustawia wartoÅ›Ä‡ biasu z konstruktora w nowej zmiennej
* nastÄ™pnie warunek sprawdza czy podano parametr wagi
* jeÅ›li sÄ… podane, przeksztaÅ‚camy je w tablicÄ™ numpy i zapisujemy do zmiennej
* w przeciwnym przypadku tworzymy losowe wagi o dÅ‚ugoÅ›ci **n_inputs** i rÃ³wnieÅ¼ zapisujemy do zmiennej

<br>

> [Neuron] def _f(self, x):

* Jest to dostarczona metoda, ktÃ³ra definiuje funkcjÄ™ aktywacji
* **x** - wartoÅ›Ä‡ wejÅ›ciowa do funkcji aktywacji
* w tym przypadku zostaÅ‚a nam podana funkcja Leaky ReLU, ktÃ³ra zwraca wartoÅ›Ä‡ **x**, jeÅ›li x jest dodatnie, a **0.1 * x**, jeÅ›li x jest ujemne
* funkcja aktywacji wprowadza nieliniowoÅ›Ä‡, co pozwala sieci neuronowej na modelowanie skomplikowanych zaleÅ¼noÅ›ci
* dziaÅ‚a jak bramka lub przeÅ‚Ä…cznik, ktÃ³ry decyduje, czy neuron wyÅ›le sygnaÅ‚ dalej

<br>

> [Neuron] def  \_\_call__(self, xs):

* Jest to dostarczona metoda, ktÃ³ra pozwala traktowaÄ‡ obiekt klasy Neuron jak funkcjÄ™
* **xs** - wejÅ›ciowe wartoÅ›ci (tablica liczb)
* fragment **np.dot()** oblicza iloczyn skalarny tablicy wejÅ›ciowej **xs** i wag **ws**
* nastÄ™pnie do wyniku iloczynu dodawany jest bias
* ostatecznie zwracany jest wynik tego obliczenia z funkcji aktywacji

<br>

> def \_\_init__(self, layers):

* Jest to konstruktor klasy ANN (Artificial Neural Network)
* **layers** - lista, ktÃ³ra okreÅ›la liczbÄ™ neuronÃ³w w kaÅ¼dej warstwie
* na poczÄ…tku inicjalizujemy pustÄ… listÄ™ warstw (kaÅ¼da warstwa to lista neuronÃ³w)
* nastÄ™pnie iterujemy przez indeksy od 1 do dÅ‚ugoÅ›ci listy layers (czyli liczby warstw)
	* w pÄ™tli tworzona jest pusta lista **layer**, ktÃ³ra bÄ™dzie przechowywaÄ‡ neurony w bieÅ¼Ä…cej warstwie
	* nastÄ™pnie iterujemy tyle razy, ile jest neuronÃ³w w bieÅ¼Ä…cej warstwie
		* w pÄ™tli tworzymy nowy neuron, przekazujÄ…c liczbÄ™ neuronÃ³w z poprzedniej warstwy jako liczbÄ™ wejÅ›Ä‡ i dodajemy go do listy **layer**
	* na koniec pierwszej pÄ™tli dodajemy caÅ‚Ä… warstwÄ™ neuronÃ³w do listy warstw **self.layers**

<br>

> def feedforward(self, inputs):

* Jest to metoda, ktÃ³ra przekazuje dane wejÅ›ciowe przez sieÄ‡ neuronowÄ…
* **inputs** - wejÅ›ciowe wartoÅ›ci
* na poczÄ…tku inicjujemy listÄ™ wejÅ›ciowych wartoÅ›ci do sieci
* nastÄ™pnie iterujemy przez kaÅ¼dÄ… warstwÄ™ w sieci neuronowej
	* w pÄ™tli tworzona jest pusta lista **new_inputs**, ktÃ³ra bÄ™dzie przechowywaÄ‡ wyjÅ›cia neuronÃ³w z bieÅ¼Ä…cej warstwy
	* nastÄ™pnie iterujemy przez kaÅ¼dy neuron w bieÅ¼Ä…cej warstwie
		* w zmiennej **output** zapisujemy obliczony wynik dla bieÅ¼Ä…cego neuronu (przekazujemy odpowiednio sformatowane ostatnie wejÅ›cie)
			* wejÅ›cie musi pasowaÄ‡ do liczby wag w bieÅ¼Ä…cym neuronie, **-1** oznacza, Å¼e liczba wierszy bÄ™dzie automatycznie obliczona aby zgadzaÅ‚a siÄ™ z liczbÄ… wag
		* dodajemy obliczony wynik do listy **new_inputs**
	* pod koniec pierwszej pÄ™tli dodajemy caÅ‚Ä… listÄ™ **new_inputs** do listy wszystkich wejÅ›Ä‡, aby moÅ¼na byÅ‚o jÄ… uÅ¼yÄ‡ jako dane wejÅ›ciowe dla nastÄ™pnej warstwy
* zwracamy wynik - ostatnie wejÅ›cie z listy (jest juÅ¼ po przejÅ›ciu przez wszystkie warstwy sieci)

<br>

> def  backpropagation(self, expected_output, learning_rate):

* Jest to metoda, ktÃ³ra aktualizuje wagi i biasy, aby zmniejszyÄ‡ bÅ‚Ä…d wyjÅ›ciowy w przyszÅ‚ych iteracjach
* **expected_output** - oczekiwane wyjÅ›cie sieci neuronowej
* **learning_rate** - wspÃ³Å‚czynnik uczenia siÄ™ (im wiÄ™kszy, tym szybciej i mniej dokÅ‚adnie)
* na poczÄ…tku inicjujemy listÄ™ **deltas** zawierajÄ…cÄ… rÃ³Å¼nicÄ™ miÄ™dzy oczekiwanym a rzeczywistym wyjÅ›ciem
* nastÄ™pnie pÄ™tla iteruje przez wszystkie warstwy sieci od koÅ„ca do poczÄ…tku
	* w **layer** ustawiamy obecnie przetwarzanÄ… warstwÄ™ neuronÃ³w
	* dalej tworzymy tablicÄ™ **next_deltas**, ktÃ³ra jest wypeÅ‚niona zerami i ma za zadanie przechowywaÄ‡ bÅ‚Ä™dy dla neuronÃ³w z poprzedniej warstwy
	* wewnÄ™trzna pÄ™tla for iteruje przez kaÅ¼dy neuron w aktualnej warstwie
		* w zmiennej **output** zapisujemy wyjÅ›cie neuronu "j" w warstwie "i + 1"
		* w zmiennej **delta** zapisujemy bÅ‚Ä…d dla neuronu
		* nastÄ™pnie aktualizujemy **next_deltas** o wpÅ‚yw bÅ‚Ä™du obecnego neuronu na poprzedniÄ… warstwÄ™
		* dalej obliczamy gradient wag w **grad_w** (jak bardzo kaÅ¼da waga powinna siÄ™ zmieniÄ‡)
		* kolejno obliczamy gradient biasu w w **grad_b** (jak bardzo bias powinien siÄ™ zmieniÄ‡)
		* pod koniec wewnÄ™trznej pÄ™tli aktualizujemy wagi i bias dla neuronu na podstawie gradientÃ³w i wspÃ³Å‚czynnika uczenia
	* natomiast pod koniec zewnÄ™trznej pÄ™tli dodajemy obliczone bÅ‚Ä™dy dla warstwy "i" do listy **deltas**, aby moÅ¼na byÅ‚o je wykorzystaÄ‡ przy nastÄ™pnym obrocie pÄ™tli

<br>

> def  train(self, X, y, epochs, learning_rate):

* Jest to metoda, ktÃ³ra odpowiada za proces uczenia sieci neuronowej
* **X** - wejÅ›ciowe dane treningowe
* **y** - oczekiwane wyjÅ›cia dla danych wejÅ›ciowych
* **epochs** - ile razy sieÄ‡ przejdzie przez caÅ‚y zbiÃ³r danych treningowych
* **learning_rate** - wspÃ³Å‚czynnik uczenia siÄ™
* na poczÄ…tku pÄ™tla iteruje przez okreÅ›lonÄ… liczbÄ™ epochs
	* wewnÄ™trzna pÄ™tla iteruje po danych treningowych (funkcja **zip()** Å‚Ä…czy elementy z dwÃ³ch list "X" i "y" w pary)
		* wywoÅ‚ujemy metodÄ™ **feedforward** Å¼eby obliczyÄ‡ wyjÅ›cia sieci dla "xi"
		* wywoÅ‚ujemy metodÄ™  **backpropagation** Å¼eby zaktualizowaÄ‡ wagi na podstawie bÅ‚Ä™dÃ³w miÄ™dzy rzeczywistym a oczekiwanym wyjÅ›ciem dla "yi"
	* wyÅ›wietlamy numer aktualnej wartoÅ›ci epoch aby Å›ledziÄ‡ postÄ™p

<br>

> def  visualize_ann(layers):

* Jest to funkcja do wizualizacji sieci neuronowej
* **layers** - lista, ktÃ³ra okreÅ›la liczbÄ™ neuronÃ³w w kaÅ¼dej warstwie
* **G** - graf skierowany utworzony za pomocÄ… biblioteki NetworkX
* **pos** - sÅ‚ownik pozycji wÄ™zÅ‚Ã³w (pary klucz-wartoÅ›Ä‡)
* **labels** - sÅ‚ownik etykiet wÄ™zÅ‚Ã³w (pary klucz-wartoÅ›Ä‡)
* **node_id** - identyfikator wÄ™zÅ‚a, licznik
***
* na samym poczÄ…tku musimy stworzyÄ‡ wÄ™zÅ‚y dla warstwy wejÅ›ciowej
* iterujemy przez liczbÄ™ neuronÃ³w w warstwie wejÅ›ciowej
	* dodajemy wÄ™zeÅ‚ do grafu z identyfikatorem **node_id**
	* ustawiamy pozycjÄ™ wÄ™zÅ‚a (w formacie wspÃ³Å‚rzÄ™dnych **x, y**)
		* wszystkie bÄ™dÄ… ustawiane pod sobÄ… w pionowej linii, bo "x" to staÅ‚a wartoÅ›Ä‡, a "y" zmienia siÄ™ w zaleÅ¼noÅ›ci od id
	* ustawiamy etykietÄ™ wÄ™zÅ‚a w formacie "Input <numer wÄ™zÅ‚a>"
	* na koniec zwiÄ™kszamy identyfikator wÄ™zÅ‚a o 1 dla kolejnych obrotÃ³w pÄ™tli
***
* nastÄ™pnie tworzymy wÄ™zÅ‚y dla warstw ukrytych
* w zmiennej **layer_count** obliczamy liczbÄ™ warstw ukrytych i wyjÅ›ciowej
* **layer_idx** - indeks bieÅ¼Ä…cej warstwy (1 dla pierwszej warstwy ukrytej itd.)
* **num_nodes** - liczba neuronÃ³w w bieÅ¼Ä…cej warstwie
* uÅ¼ywamy **enumerate()** aby mÃ³c iterowaÄ‡ przez elementy listy i mieÄ‡ dostÄ™p do ich indeksÃ³w
* w tym przypadku iterujemy przez listÄ™ **layers** od elementu z indeksem 1, do elementu przedostatniego (co okreÅ›la [1:-1] oraz start = 1)
* dziaÅ‚amy tak jak poprzednio, dodajÄ…c wÄ™zÅ‚y do grafu, ustalajÄ…c ich kolejne pozycje, nazwy oraz zwiÄ™kszajÄ…c identyfikator
	* w pos[node_id] znajduje siÄ™ teraz **layer_idx**, co pozwoli na uÅ‚oÅ¼enie kaÅ¼dej z warstw kolumnami przyrastajÄ…cymi w dÃ³Å‚
***
* dalej tworzymy wÄ™zeÅ‚ dla warstwy wyjÅ›ciowej
* iterujemy po liczbie neuronÃ³w w warstwie wyjÅ›ciowej (ostatni element listy **layers**)
* dziaÅ‚amy tak jak poprzednio, dodajÄ…c wÄ™zÅ‚y do grafu, ustalajÄ…c ich kolejne pozycje, nazwy oraz zwiÄ™kszajÄ…c identyfikator
	* w pos[node_id] znajduje siÄ™ teraz **layer_count**, ktÃ³ry ustawia wÄ™zÅ‚y wyjÅ›ciowe na samym koÅ„cu wykresu (w tym zadaniu jest to i tak tylko jeden wÄ™zeÅ‚)
***
* ostatecznie naleÅ¼y dodaÄ‡ krawÄ™dzie miÄ™dzy wÄ™zÅ‚ami w warstwach
* **prev_layer_nodes** - przechowuje numeracje wÄ™zÅ‚Ã³w z poprzedniej warstwy
	* poczÄ…tkowo przypisujemy jej sekwencjÄ™ liczb od 0 do "layers[0] - 1" uÅ¼ywajÄ…c metody range(), co odpowiada liczbie neuronÃ³w w warstwie wejÅ›ciowej
* **current_layer_start** - okreÅ›la indeks poczÄ…tkowy wÄ™zÅ‚Ã³w w aktualnej warstwie
	* poczÄ…tkowo przypisujemy jej wartoÅ›Ä‡ layers[0], dziÄ™ki czemu zaczynamy od pierwszego wÄ™zÅ‚a w warstwie wejÅ›ciowej

<br>

* zaczynamy od iterowania przez warstwy sieci neuronowej (z uÅ¼yciem [1:] poniewaÅ¼ juÅ¼ obsÅ‚uÅ¼yliÅ›my warstwÄ™ wejÅ›ciowÄ…)
	* w zmiennej **current_layer_nodes** przechowujemy numeracje wÄ™zÅ‚Ã³w aktualnej warstwy
	* dalej rozpoczynamy pÄ™tlÄ™ przez wÄ™zÅ‚y z poprzedniej warstwy, aby utworzyÄ‡ krawÄ™dzie
		* wewnÄ™trzna pÄ™tla iteruje przez wÄ™zÅ‚y z aktualnej warstwy
		* dodajemy krawÄ™dÅº miÄ™dzy wÄ™zÅ‚em z **prev_node** a **curr_node**
	* pod koniec pÄ™tli po layers aktualizujemy **prev_layer_nodes** na wartoÅ›Ä‡ obecnie rozpatrywanej warstwy na potrzeby kolejnego obrotu
	* aktualizujemy rÃ³wnieÅ¼ **current_layer_start** o wartoÅ›Ä‡ **layer_size** aby wskazaÄ‡ na pierwszy wÄ™zeÅ‚ w nastÄ™pnej warstwie
***
* w koÅ„cu w funkcji rysujemy graf G na podstawie okreÅ›lonych parametrÃ³w za pomocÄ… nx.draw()
* uÅ¼ywajÄ…c plt.show() wyÅ›wietlamy wykres

<br>
<br>

Pod koniec programu ustawiamy parametry warstw, aby byÅ‚y zgodne z dostarczonÄ… strukturÄ… ze zdjÄ™cia. Inicjalizujemy sieÄ‡ neuronowÄ…, tworzymy przykÅ‚adowe inputy, outputy oraz wartoÅ›Ä‡ do przetestowania. WyÅ›wietlamy rÃ³wnieÅ¼ zwizualizowanÄ… strukturÄ™.
