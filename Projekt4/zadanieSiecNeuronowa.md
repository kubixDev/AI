# ğŸ“— Projekt 4 - sztuczne sieci neuronowe


## â­ Zadanie 1
WykorzystujÄ…c kod klasy Neuron zaimplementuj sieÄ‡ neuronowÄ… z poniÅ¼szÄ… strukturÄ… oraz dostarcz kod do wizualizacji struktury sieci.

<br>

**Dostarczony kod:**

```
import numpy as np 

class Neuron:  
    def __init__(self, n_inputs, bias = 0., weights = None):  
        self.b = bias
        if weights: self.ws = np.array(weights)
        else: self.ws = np.random.rand(n_inputs)

    def _f(self, x):                                            # Activation function (here: leaky_relu)
        return max(x*.1, x)   

    def __call__(self, xs):                                     # Calculate the neuron's output:
                return self._f(xs @ self.ws + self.b)           #   - multiply the inputs with the weights and sum the values together
                                                                #   - add the bias value
                                                                #   - then transform the value via an activation function
```

<br>

**Implementacja Neuron wraz z sieciÄ… neuronowÄ… ANN:**

```
import numpy as np
import matplotlib.pyplot as plt
import networkx as nx

##############################
# Dostarczona klasa Neuron
##############################
class Neuron:
    def __init__(self, n_inputs, bias = 0.0, weights = None):       # Konstruktor klasy Neuron
        self.b = bias

        if weights is not None:
            self.ws = np.array(weights)
        else:
            self.ws = np.random.rand(n_inputs)


    def _f(self, x):                                                # Metoda definiujaca funkcje aktywacji
        return max(0.1 * x, x)                                      # z tabelki jest to Leaky ReLU


    def __call__(self, xs):                                         # Pozwala na wywolanie obiektu klasy
        return self._f(np.dot(xs, self.ws) + self.b)                # neuron jak funkcji



########################################################
# Implementacja klasy ANN (Artificial Neural Network)
########################################################
class ANN:
    def __init__(self, layers):                                     # Konstruktor klasy ANN
        self.layers = []
        
        for i in range(1, len(layers)):
            layer = []

            for _ in range(layers[i]):
                neuron = Neuron(layers[i - 1])
                layer.append(neuron)

            self.layers.append(layer)



    def feedforward(self, inputs):                                  # Metoda, ktora przekazuje dane
        for layer in self.layers:                                   # wejsciowe przez siec neuronowa
            new_inputs = []

            for neuron in layer:
                output = neuron.feedforward(inputs)
                new_inputs.append(output)

            inputs = np.array(new_inputs)

        return inputs



###############################
# Wizualizacja ANN z obrazka
###############################
def visualize_ann(layers):
    G = nx.DiGraph()
    pos = {}
    labels = {}
    node_id = 0


    # Tworzenie wezlow dla warstwy wejsciowej (input layer)
    for i in range(layers[0]):
        G.add_node(node_id)
        pos[node_id] = (0, -i)
        labels[node_id] = f"Input {i+1}"
        node_id = node_id + 1


    # Tworzenie wezlow dla ukrytych warstw (hidden layers)
    layer_count = len(layers) - 1

    for layer_idx, num_nodes in enumerate(layers[1:-1], start = 1):
        for i in range(num_nodes):
            G.add_node(node_id)
            pos[node_id] = (layer_idx, -i)
            labels[node_id] = f"Hidden{layer_idx} {i+1}"
            node_id = node_id + 1


    # Tworzenie wezlow dla warstwy wyjsciowej (output layer)
    for i in range(layers[-1]):
        G.add_node(node_id)
        pos[node_id] = (layer_count, -i)
        labels[node_id] = f"Output {i+1}"
        node_id = node_id + 1


    # Dodawanie krawedzi miedzy warstwami
    prev_layer_nodes = range(layers[0])
    current_layer_start = layers[0]

    for layer_size in layers[1:]:
        current_layer_nodes = range(current_layer_start, current_layer_start + layer_size)

        for prev_node in prev_layer_nodes:
            for curr_node in current_layer_nodes:
                G.add_edge(prev_node, curr_node)

        prev_layer_nodes = current_layer_nodes
        current_layer_start = current_layer_start + layer_size


    # Rysowanie sieci przy uzyciu biblioteki NetworkX
    nx.draw(G, pos, labels = labels, with_labels = True, node_size = 3000, node_color = 'lightgreen', font_size = 8)
    plt.show()



######################################
# Wybor parametrow i uzycie funkcji
######################################
input_size = 3
hidden_layer_1_size = 4
hidden_layer_2_size = 4
output_size = 1
layers = [input_size, hidden_layer_1_size, hidden_layer_2_size, output_size]

# Inicjalizacja sztucznej sieci neuronowej
ann = ANN(layers)

# Wizualizacja struktury
visualize_ann(layers)
 ```

<br>

****Importy:****

* **numpy** - do wykonywania operacji matematycznych
*  **matplotlib.pyplot** - do rysowania wykresÃ³w
*  **networkx** - do tworzenia i wizualizacji grafÃ³w

<br>

****WyjaÅ›nienie:****

> \[Neuron] def  \_\_init__(self, n_inputs, bias = 0.0, weights = None):

* Jest to dostarczony konstruktor klasy Neuron
* **n_inputs** - liczba wejÅ›Ä‡ dla neuronu
* **bias** - przesuniÄ™cie neuronu, domyÅ›lnie "0.0"
* **weights** - wagi dla wejÅ›Ä‡ neuronu, domyÅ›lnie "None"
* na poczÄ…tek ustawia wartoÅ›Ä‡ biasu z konstruktora w nowej zmiennej
* nastÄ™pnie warunek sprawdza czy podano parametr wagi
* jeÅ›li sÄ… podane, przeksztaÅ‚camy je w tablicÄ™ numpy i zapisujemy do zmiennej
* w przeciwnym przypadku tworzymy losowe wagi o dÅ‚ugoÅ›ci **n_inputs** i rÃ³wnieÅ¼ zapisujemy do zmiennej

<br>

> \[Neuron] def _f(self, x):

* Jest to dostarczona metoda, ktÃ³ra definiuje funkcjÄ™ aktywacji
* **x** - wartoÅ›Ä‡ wejÅ›ciowa do funkcji aktywacji
* w tym przypadku zostaÅ‚a nam podana funkcja Leaky ReLU, ktÃ³ra zwraca wartoÅ›Ä‡ **x**, jeÅ›li x jest dodatnie, a **0.1 * x**, jeÅ›li x jest ujemne
* funkcja aktywacji wprowadza nieliniowoÅ›Ä‡, co pozwala sieci neuronowej na modelowanie skomplikowanych zaleÅ¼noÅ›ci
* dziaÅ‚a jak bramka lub przeÅ‚Ä…cznik, ktÃ³ry decyduje, czy neuron wyÅ›le sygnaÅ‚ dalej

<br>

> \[Neuron] def  \_\_call__(self, xs):

* Jest to dostarczona metoda, ktÃ³ra pozwala traktowaÄ‡ obiekt klasy Neuron jak funkcjÄ™
* **xs** - wejÅ›ciowe wartoÅ›ci (tablica liczb)
* fragment **np.dot()** oblicza iloczyn skalarny tablicy wejÅ›ciowej **xs** i wag **ws**
* nastÄ™pnie do wyniku iloczynu dodawany jest bias
* ostatecznie zwracany jest wynik tego obliczenia z funkcji aktywacji

<br>

> def \_\_init__(self, layers):

* Jest to konstruktor klasy ANN (Artificial Neural Network)
* **layers** - lista, ktÃ³ra okreÅ›la liczbÄ™ neuronÃ³w w kaÅ¼dej warstwie
* na poczÄ…tku inicjalizujemy pustÄ… listÄ™ warstw (kaÅ¼da warstwa to lista neuronÃ³w)
* nastÄ™pnie iterujemy przez indeksy od 1 do dÅ‚ugoÅ›ci listy layers (czyli liczby warstw)
	* w pÄ™tli tworzona jest pusta lista **layer**, ktÃ³ra bÄ™dzie przechowywaÄ‡ neurony w bieÅ¼Ä…cej warstwie
	* nastÄ™pnie iterujemy tyle razy, ile jest neuronÃ³w w bieÅ¼Ä…cej warstwie
		* w pÄ™tli tworzymy nowy neuron, przekazujÄ…c liczbÄ™ neuronÃ³w z poprzedniej warstwy jako liczbÄ™ wejÅ›Ä‡ i dodajemy go do listy **layer**
	* na koniec pierwszej pÄ™tli dodajemy caÅ‚Ä… warstwÄ™ neuronÃ³w do listy warstw **self.layers**

<br>

> def feedforward(self, inputs):

* Jest to metoda, ktÃ³ra przekazuje dane wejÅ›ciowe przez sieÄ‡ neuronowÄ…
* **inputs** - wejÅ›ciowe wartoÅ›ci
* na poczÄ…tku iterujemy przez kaÅ¼dÄ… warstwÄ™ w sieci neuronowej
	* w pÄ™tli tworzona jest pusta lista **new_inputs**, ktÃ³ra bÄ™dzie przechowywaÄ‡ wyniki (wyjÅ›cia) neuronÃ³w z bieÅ¼Ä…cej warstwy
	* nastÄ™pnie iterujemy przez kaÅ¼dy neuron w bieÅ¼Ä…cej warstwie
		* w zmiennej **output** zapisujemy wyjÅ›cie neuronu (po przekazaniu danych wejÅ›ciowych inputs, wywoÅ‚ujÄ…c metodÄ™ \_\_call__)
		* dodajemy obliczony wynik do listy new_inputs
	* na koniec pÄ™tli konwertujemy listÄ™ **new_inputs** na tablicÄ™ numpy, aby moÅ¼na byÅ‚o jÄ… uÅ¼yÄ‡ jako dane wejÅ›ciowe dla nastÄ™pnej warstwy
* ostatecznie zwracamy wynik po przejÅ›ciu przez wszystkie warstwy sieci

<br>

> def  visualize_ann(layers):

* Jest to funkcja do wizualizacji sieci neuronowej
* **layers** - lista, ktÃ³ra okreÅ›la liczbÄ™ neuronÃ³w w kaÅ¼dej warstwie
* **G** - graf skierowany utworzony za pomocÄ… biblioteki NetworkX
* **pos** - sÅ‚ownik pozycji wÄ™zÅ‚Ã³w (pary klucz-wartoÅ›Ä‡)
* **labels** - sÅ‚ownik etykiet wÄ™zÅ‚Ã³w (pary klucz-wartoÅ›Ä‡)
* **node_id** - identyfikator wÄ™zÅ‚a, licznik
***
* na samym poczÄ…tku musimy stworzyÄ‡ wÄ™zÅ‚y dla warstwy wejÅ›ciowej
* iterujemy przez liczbÄ™ neuronÃ³w w warstwie wejÅ›ciowej
	* dodajemy wÄ™zeÅ‚ do grafu z identyfikatorem **node_id**
	* ustawiamy pozycjÄ™ wÄ™zÅ‚a (w formacie wspÃ³Å‚rzÄ™dnych **x, y**)
		* wszystkie bÄ™dÄ… ustawiane pod sobÄ… w pionowej linii, bo "x" to staÅ‚a wartoÅ›Ä‡, a "y" zmienia siÄ™ w zaleÅ¼noÅ›ci od id
	* ustawiamy etykietÄ™ wÄ™zÅ‚a w formacie "Input <numer wÄ™zÅ‚a>"
	* na koniec zwiÄ™kszamy identyfikator wÄ™zÅ‚a o 1 dla kolejnych obrotÃ³w pÄ™tli
***
* nastÄ™pnie tworzymy wÄ™zÅ‚y dla warstw ukrytych
* w zmiennej **layer_count** obliczamy liczbÄ™ warstw ukrytych i wyjÅ›ciowej
* **layer_idx** - indeks bieÅ¼Ä…cej warstwy (1 dla pierwszej warstwy ukrytej itd.)
* **num_nodes** - liczba neuronÃ³w w bieÅ¼Ä…cej warstwie
* uÅ¼ywamy **enumerate()** aby mÃ³c iterowaÄ‡ przez elementy listy i mieÄ‡ dostÄ™p do ich indeksÃ³w
* w tym przypadku iterujemy przez listÄ™ **layers** od elementu z indeksem 1, do elementu przedostatniego (co okreÅ›la \[1:-1] oraz start = 1)
* dziaÅ‚amy tak jak poprzednio, dodajÄ…c wÄ™zÅ‚y do grafu, ustalajÄ…c ich kolejne pozycje, nazwy oraz zwiÄ™kszajÄ…c identyfikator
	* w pos\[node_id] znajduje siÄ™ teraz **layer_idx**, co pozwoli na uÅ‚oÅ¼enie kaÅ¼dej z warstw kolumnami przyrastajÄ…cymi w dÃ³Å‚
***
* dalej tworzymy wÄ™zeÅ‚ dla warstwy wyjÅ›ciowej
* iterujemy po liczbie neuronÃ³w w warstwie wyjÅ›ciowej (ostatni element listy **layers**)
* dziaÅ‚amy tak jak poprzednio, dodajÄ…c wÄ™zÅ‚y do grafu, ustalajÄ…c ich kolejne pozycje, nazwy oraz zwiÄ™kszajÄ…c identyfikator
	* w pos\[node_id] znajduje siÄ™ teraz **layer_count**, ktÃ³ry ustawia wÄ™zÅ‚y wyjÅ›ciowe na samym koÅ„cu wykresu (w tym zadaniu jest to i tak tylko jeden wÄ™zeÅ‚)
***
* ostatecznie naleÅ¼y dodaÄ‡ krawÄ™dzie miÄ™dzy wÄ™zÅ‚ami w warstwach
* **prev_layer_nodes** - przechowuje numeracje wÄ™zÅ‚Ã³w z poprzedniej warstwy
	* poczÄ…tkowo przypisujemy jej sekwencjÄ™ liczb od 0 do "layers\[0] - 1" uÅ¼ywajÄ…c metody range(), co odpowiada liczbie neuronÃ³w w warstwie wejÅ›ciowej
* **current_layer_start** - okreÅ›la indeks poczÄ…tkowy wÄ™zÅ‚Ã³w w aktualnej warstwie
	* poczÄ…tkowo przypisujemy jej wartoÅ›Ä‡ layers[0], dziÄ™ki czemu zaczynamy od pierwszego wÄ™zÅ‚a w warstwie wejÅ›ciowej
<br>

* zaczynamy od iterowania przez warstwy sieci neuronowej (z uÅ¼yciem \[1:] poniewaÅ¼ juÅ¼ obsÅ‚uÅ¼yliÅ›my warstwÄ™ wejÅ›ciowÄ…)
	* w zmiennej **current_layer_nodes** przechowujemy numeracje wÄ™zÅ‚Ã³w aktualnej warstwy
	* dalej rozpoczynamy pÄ™tlÄ™ przez wÄ™zÅ‚y z poprzedniej warstwy, aby utworzyÄ‡ krawÄ™dzie
		* wewnÄ™trzna pÄ™tla iteruje przez wÄ™zÅ‚y z aktualnej warstwy
		* dodajemy krawÄ™dÅº miÄ™dzy wÄ™zÅ‚em z **prev_node** a **curr_node**
	* pod koniec pÄ™tli po layers aktualizujemy **prev_layer_nodes** na wartoÅ›Ä‡ obecnie rozpatrywanej warstwy na potrzeby kolejnego obrotu
	* aktualizujemy rÃ³wnieÅ¼ **current_layer_start** o wartoÅ›Ä‡ **layer_size** aby wskazaÄ‡ na pierwszy wÄ™zeÅ‚ w nastÄ™pnej warstwie
***
* w koÅ„cu w funkcji rysujemy graf G na podstawie okreÅ›lonych parametrÃ³w za pomocÄ… nx.draw()
* uÅ¼ywajÄ…c plt.show() wyÅ›wietlamy wykres
