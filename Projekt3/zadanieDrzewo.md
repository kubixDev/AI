 # ğŸ“— Projekt 3 - drzewa decyzyjne


## â­ Zadanie 1
Zaimplementuj klasÄ™ DecisionTree, ktÃ³ra moÅ¼e byÄ‡ uÅ¼yta w poniÅ¼szym skrypcie do treningu

<br>

**Dostarczony skrypt do testu dziaÅ‚ania:**

```
from sklearn import datasets
from sklearn.model_selection import train_test_split
from decision_tree import DecisionTree
import numpy as np

data = datasets.load_iris()
X, y = data.data, data.target

X_train, X_test, y_train, y_test = train_test_split(
	X, y, test_size=0.2, random_state=1234
)

clf = DecisionTree(max_depth=10)
clf.fit(X_train, y_train)
predictions = clf.predict(X_test)

def accuracy(y_test, y_pred):
	return np.sum(y_test == y_pred) / len(y_test)

acc = accuracy(y_test, predictions)
print(acc)
```

<br>

**Implementacja DecisionTree:**

```
import numpy as np
from collections import Counter

##############################
# Wezel w drzewie decyzyjnym
##############################
class Node:
    def __init__(self, feature = None, threshold = None, left = None, right = None, *, value = None):
        self.feature = feature       # zmienna na podstawie ktorej dokonujemy podzialu w drzewie
        self.threshold = threshold   # prog podzialu
        self.left = left             # lewe poddrzewo
        self.right = right           # prawe poddrzewo
        self.value = value           # wartosc w lisciu (jesli to lisc)

    def is_leaf_node(self):
        return self.value is not None


####################################
# Implementacja drzewa decyzyjnego
####################################
class DecisionTree:
    def __init__(self, min_samples_split = 2, max_depth = 100, n_features = None):
        self.min_samples_split = min_samples_split   # minimalna liczba probek do podzialu wezla
        self.max_depth = max_depth                   # maksymalna glebokosc drzewa
        self.n_features = n_features                 # liczba cech branych pod uwage przy kazdym podziale
        self.root = None                             # korzen drzewa


    #######################################
    # Metoda inicjujaca trenowanie modelu 
    #######################################
    def fit(self, X, y):
        if self.n_features is None:
            self.n_features = X.shape[1]                          # domyslnie liczba cech rowna liczbie cech w danych
        else:
            self.n_features = min(self.n_features, X.shape[1])    # jesli podano inna liczbe cech, to bierzemy minimum

        self.root = self._grow_tree(X, y)


    ##########################################
    # Metoda do budowania drzewa decyzyjnego
    ##########################################
    def _grow_tree(self, X, y, depth = 0):
        n_samples, n_features = X.shape
        n_labels = len(np.unique(y))

                                                                   # Sprawdzamy kryteria zatrzymania:
        if (depth >= self.max_depth                                #   - czy osiagnelismy maksymalna glebokosc
                or n_samples < self.min_samples_split              #   - czy liczba probek jest mniejsza niz minimalna wymagana do podzialu
                or n_labels == 1):                                 #   - czy wszystkie probki naleza do tej samej etykiety
            leaf_value = Counter(y).most_common(1)[0][0]
            return Node(value=leaf_value)                          # zwracamy wartosc najczesciej wystepujacej etykiety


        # znajdowanie najlepszego podzialu
        feat_idxs = np.random.choice(n_features, self.n_features, replace=False)
        best_feature, best_threshold = self._best_split(X, y, feat_idxs)

        # podzial zbioru danych
        left_idxs, right_idxs = self._split(X[:, best_feature], best_threshold)
        left = self._grow_tree(X[left_idxs, :], y[left_idxs], depth + 1)
        right = self._grow_tree(X[right_idxs, :], y[right_idxs], depth + 1)
        
        return Node(best_feature, best_threshold, left, right)


    ####################################################
    # Metoda szukajaca najlepszego podzialu dla danych
    ####################################################
    def _best_split(self, X, y, feat_idxs):
        best_feature, best_threshold = None, None
        best_gain = -1

        for feat_idx in feat_idxs:
            X_column = X[:, feat_idx]
            thresholds = np.unique(X_column)

            for threshold in thresholds:
                gain = self._information_gain(y, X_column, threshold)

                if gain > best_gain:
                    best_feature = feat_idx
                    best_threshold = threshold
                    best_gain = gain


        return best_feature, best_threshold


    ############################################################
    # Metoda obliczajaca zysk informacyjny dla podziaÅ‚u danych
    ############################################################
    def _information_gain(self, y, X_column, threshold):
        left_idxs, right_idxs = self._split(X_column, threshold)

        if len(left_idxs) == 0 or len(right_idxs) == 0:
            return 0

        y_left = y[left_idxs]
        y_right = y[right_idxs]
        entropy_left = self._entropy(y_left)
        entropy_right = self._entropy(y_right)
        entropy_split = (len(left_idxs) * entropy_left + len(right_idxs) * entropy_right) / len(y)
        entropy_total = self._entropy(y)

        return entropy_total - entropy_split


    ###########################################
    # Metoda dzielaca dane na podstawie progu
    ###########################################
    def _split(self, X_column, split_thresh):
        left_idxs = np.where(X_column <= split_thresh)[0]
        right_idxs = np.where(X_column > split_thresh)[0]

        return left_idxs, right_idxs


    ##########################################################
    # Metoda obliczajaca entropie dla danego zestawu etykiet
    ##########################################################
    def _entropy(self, y):
        hist = np.bincount(y)
        ps = hist / len(y)

        return -np.sum([p * np.log(p) for p in ps if p > 0])


    ####################################
    # Metoda przechodzaca przez drzewo
    ####################################
    def _traverse_tree(self, x, node):
        if node.is_leaf_node():
            return node.value

        if x[node.feature] <= node.threshold:
            return self._traverse_tree(x, node.left)
        
        else: return self._traverse_tree(x, node.right)


    ############################################################
    # Metoda przewidujaca etykiety dla danego zestawu probek X
    ############################################################
    def predict(self, X):
        return np.array([self._traverse_tree(x, self.root) for x in X])
 ```

<br>

****WyjaÅ›nienie:****

> def fit(self, X, y):

* Jest to metoda, ktÃ³ra inicjuje trenowanie modelu drzewa decyzyjnego
* **X** - dane treningowe, macierz (zbiÃ³r cech)
* **y** - odpowiadajÄ…ce im etykiety (oczekiwane wartoÅ›ci)
* sprawdzamy czy uÅ¼ytkownik podaÅ‚ wartoÅ›Ä‡ **n_features** (liczbÄ™ cech)
* w pierwszym przypadku, kiedy nie zostaÅ‚a podana Å¼adna wartoÅ›Ä‡, to liczba cech jest ustawiana na liczbÄ™ wszystkich cech w danych **X**
* w drugim przypadku, kiedy wpiszemy wÅ‚asnÄ… iloÅ›Ä‡, zostanie przypisana mniejsza wartoÅ›Ä‡ (wÅ‚asna iloÅ›Ä‡/iloÅ›Ä‡ cech w danych X), aby nie przekroczyÄ‡ z gÃ³ry ustalonej liczby cech z danych **X**
* fragment **X.shape[1]** zwraca liczbÄ™ kolumn w macierzy danych **X**. Wiersz zawiera jednÄ… prÃ³bkÄ™ danych, a kolumna konkretnÄ… cechÄ™
* na koÅ„cu przypisujemy do korzenia wynik metody grow_tree, ktÃ³ra zwraca zbudowane drzewo decyzyjne

<br>

> def _grow_tree(self, X, y, depth=0):

* Jest to metoda, ktÃ³ra odpowiada za budowanie drzewa
* na poczÄ…tku przypisujemy liczbÄ™ prÃ³bek oraz liczbÄ™ cech do odpowiadajÄ…cych im zmiennych przy uÅ¼yciu **X.shape** (zwraca krotkÄ™ typu: liczba wierszy, liczba kolumn)
* zmienna **n_labels** zawiera iloÅ›Ä‡ unikalnych etykiet, im wiÄ™ksza liczba, tym lepiej - oznacza to wiÄ™kszÄ… rÃ³Å¼norodnoÅ›Ä‡ dostarczonych danych
* instrukcja if sprawdza warunki zatrzymania rekurencyjnej budowy drzewa
	* jeÅ¼eli osiÄ…gnÄ™liÅ›my z gÃ³ry ustalonÄ… gÅ‚Ä™bokoÅ›Ä‡ **max_depth**
	* albo jeÅ¼eli liczba prÃ³bek **n_samples** jest zbyt maÅ‚a, aby kontynuowaÄ‡ podziaÅ‚
	* albo jeÅ¼eli liczba unikalnych etykiet **n_labels** wynosi 1, co oznaczaÅ‚oby Å¼e nie sÄ… unikalne i podziaÅ‚ nie ma sensu
* nastÄ™pnie w Å›rodku ifa przypisujemy do **leaf_value** najczÄ™Å›ciej wystÄ™pujÄ…cÄ… etykietÄ™ (przy pomocy licznika Counter) i zwracamy to jako wartoÅ›Ä‡ liÅ›cia drzewa
* w przypadku kiedy warunki koÅ„ca nie zostaÅ‚y speÅ‚nione, przypisujemy do tablicy indeksÃ³w **feat_idxs** losowo pewnÄ… liczbÄ™ cech, ktÃ³re bÄ™dÄ… brane pod uwagÄ™ podczas budowy drzewa
* nastÄ™pnie metoda **_best_split** znajduje najlepszÄ… cechÄ™ (best_feature) oraz najlepszy prÃ³g podziaÅ‚u (best_threshold) spoÅ›rÃ³d wylosowanych cech
* dalej dzielimy dane na dwie grupy na podstawie wybranej cechy i progu
	* jedna grupa dla prÃ³bek, gdzie wartoÅ›Ä‡ tej cechy jest mniejsza lub rÃ³wna progowi
	* druga grupa dla pozostaÅ‚ych prÃ³bek
* potem dla obu grup budujemy rekurencyjnie osobne poddrzewo decyzyjne
* na koÅ„cu tworzymy wÄ™zeÅ‚ drzewa zawierajÄ…cy najlepszÄ… cechÄ™, najlepszy prÃ³g podziaÅ‚u oraz odnoÅ›niki do lewego i prawego poddrzewa

<br>

> def _best_split(self, X, y, feat_idxs):

* Jest to metoda, ktÃ³ra znajduje najlepszy podziaÅ‚ danych wejÅ›ciowych
* inicjalizujemy best_feature oraz best_threshold
* ustawiajÄ…c **best_gain** na wartoÅ›Ä‡ ujemnÄ… mamy pewnoÅ›Ä‡, Å¼e kaÅ¼dy dodatni (czyli poprawny) zysk informacyjny bÄ™dzie od tego wiÄ™kszy
* nastÄ™pnie iterujemy po kaÅ¼dym indeksie cechy
* dla kaÅ¼dej cechy pobieramy kolumnÄ™ danych z macierzy **X**
* potem znajdujemy unikalne wartoÅ›ci, ktÃ³re posÅ‚uÅ¼Ä… jako progi podziaÅ‚u
* w koÅ„cu, dla kaÅ¼dego progu podziaÅ‚u obliczamy zysk informacyjny
* na otrzymanej wartoÅ›ci realizujemy algorytm max (ktÃ³ry zamieni wartoÅ›ci, gdy nowo obliczony gain bÄ™dzie wiÄ™kszy od obecnie najwiÄ™kszego)
* ostatecznie zwracamy najlepszy podziaÅ‚

<br>

> def _information_gain(self, y, X_column, threshold):

* Jest to metoda, ktÃ³ra oblicza zysk informacyjny dla podziaÅ‚u danych
* na poczÄ…tku znowu dzielimy dane na dwie grupy na podstawie progu podziaÅ‚u
	* jedna grupa dla prÃ³bek, gdzie wartoÅ›Ä‡ cechy jest mniejsza lub rÃ³wna progowi
	* druga grupa dla pozostaÅ‚ych prÃ³bek
* warunek if sprawdza sytuacjÄ™, w ktÃ³rej po podziale ktÃ³raÅ› z grup jest pusta (czyli nie zawiera Å¼adnych prÃ³bek) - w takiej sytuacji zwracamy 0, czyli **brak zysku** informacyjnego
* jeÅ¼eli jednak tak siÄ™ nie staÅ‚o, obliczamy entropiÄ™ dla kaÅ¼dej z tych dwÃ³ch grup
	* **y_left** oraz **y_right** przypisujÄ… etykiety dla prÃ³bek po lewej i prawej stronie podziaÅ‚u
	* **entropy_left** oraz **entropy_right** obliczajÄ… entropiÄ™ dla etykiet w odpowiednich grupach
	* **entropy_split** oblicza Å›redniÄ… waÅ¼onÄ… z entropii obu grup po podziale
	* **entropy_total** oblicza poczÄ…tkowÄ… entropiÄ™ przed podziaÅ‚em danych
* wynik to zysk informacyjny (rÃ³Å¼nica miÄ™dzy entropiÄ… przed i po podziale danych)
* jeÅ¼eli rÃ³Å¼nica jest duÅ¼a, to znaczy Å¼e podziaÅ‚ byÅ‚ skuteczny

<br>

> def _split(self, X_column, split_thresh):

* Jest to metoda, ktÃ³ra odpowiada za podziaÅ‚ danych na podstawie progu dla konkretnej cechy
* funkcja **np.where** przeszukuje tablicÄ™ i zwraca indeksy elementÃ³w, dla ktÃ³rych warunek jest speÅ‚niony
* dokonujemy podziaÅ‚u indeksÃ³w prÃ³bek na dwie grupy na podstawie progu
	* **left_idxs** znajduje indeksy elementÃ³w w **X_column**, ktÃ³re sÄ… mniejsze lub rÃ³wne niÅ¼ wartoÅ›Ä‡ progu podziaÅ‚u
	* **right_idxs** znajduje indeksy elementÃ³w w **X_column**, ktÃ³re sÄ… wiÄ™ksze niÅ¼ wartoÅ›Ä‡ progu podziaÅ‚u
* konieczne jest dodanie **[0]** na koÅ„cu, poniewaÅ¼ domyÅ›lnie np.where zwraca krotkÄ™ z jednym elementem, a po dodaniu zamienimy to na zwykÅ‚Ä… tablicÄ™ z indeksami
* na koniec zwracamy te tablice

<br>

> def _entropy(self, y):

* Jest to dostarczona metoda, ktÃ³ra oblicza entropiÄ™ dla podanego zestawu etykiet
* entropia to miara nieuporzÄ…dkowania w zbiorze danych, **mierzy rÃ³Å¼norodnoÅ›Ä‡ etykiet**
* im wiÄ™ksza entropia, tym bardziej zrÃ³Å¼nicowane mamy etykiety
* natomiast entropia rÃ³wna zero oznacza, Å¼e wszystkie etykiety w zbiorze sÄ… identyczne
* jest obliczana z gotowego schematu
	* najpierw obliczamy iloÅ›Ä‡ wystÄ…pieÅ„ kaÅ¼dej etykiety w zbiorze danych
	* nastÄ™pnie obliczamy prawdopodobieÅ„stwo wystÄ…pienia kaÅ¼dej etykiety
	* uÅ¼ywamy wzoru i sumujemy wyniki dla wszystkich etykiet
* wynik to obliczona entropia 

<br>

> def _traverse_tree(self, x, node):

* Jest to metoda, ktÃ³ra przechodzi przez drzewo w celu przewidzenia etykiety dla okreÅ›lonej prÃ³bki
* **x** - prÃ³bka, dla ktÃ³rej chcemy przewidzieÄ‡ etykietÄ™
* **node** - bieÅ¼Ä…cy wÄ™zeÅ‚, od ktÃ³rego rozpoczynamy przeszukiwanie
* poczÄ…tkowa instrukcja if sprawdza, czy bieÅ¼Ä…cy wÄ™zeÅ‚ jest liÅ›ciem (czyli elementem skrajnym drzewa, z ktÃ³rym nic juÅ¼ siÄ™ nie Å‚Ä…czy)
* jeÅ¼eli tak, to od razu moÅ¼emy zwrÃ³ciÄ‡ wartoÅ›Ä‡ tego wÄ™zÅ‚a
* w przeciwnym wypadku sprawdzamy czy prÃ³bka **x** speÅ‚nia warunek podziaÅ‚u dla danej cechy i progu
* w zaleÅ¼noÅ›ci od wyniku przechodzimy do lewego lub prawego poddrzewa
* powtarzamy to rekurencyjnie aÅ¼ dotrzemy do liÅ›cia i zwrÃ³cimy jego wartoÅ›Ä‡

<br>

> def predict(self, X):

* Jest to dostarczona metoda, ktÃ³ra sÅ‚uÅ¼y do przewidywania etykiet dla zestawu prÃ³bek, uÅ¼ywajÄ…ca wytrenowanego drzewa decyzyjnego
* iterujemy przez kaÅ¼dÄ… prÃ³bkÄ™ w zestawie **X**
* dla kaÅ¼dej prÃ³bki przewidujemy etykietÄ™ (metoda **_traverse_tree**) 
* na koÅ„cu zbieramy i zwracamy przewidziane etykiety w tablicy
